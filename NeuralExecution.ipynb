{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraph(Data):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph_type = None\n",
    "        self.adj_list = None\n",
    "        self.num_nodes = 0\n",
    "        self.nx_graph = nx.Graph()\n",
    "    \n",
    "    def create_ladder(self, n):\n",
    "        self.graph_type = 'ladder'\n",
    "        self.num_nodes = 2 * n\n",
    "        edges = [(0, 1), (1, 0)]\n",
    "        nx_edges = [(0, 1)]\n",
    "        \n",
    "        for i in range(2, 2 * n, 2):\n",
    "            edges.append((i, i + 1))\n",
    "            edges.append((i + 1, i))\n",
    "            nx_edges.append((i, i + 1))\n",
    "            edges.append((i, i - 2))\n",
    "            edges.append((i - 2, i))\n",
    "            nx_edges.append((i, i - 2))\n",
    "            edges.append((i + 1, i - 1))\n",
    "            edges.append((i - 1, i + 1))\n",
    "            nx_edges.append((i + 1, i - 1))\n",
    "            \n",
    "        edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "        self.edge_index = edge_index.t().contiguous()\n",
    "        self.nx_graph.add_edges_from(nx_edges)\n",
    "        \n",
    "        self.initialize_node_attr()\n",
    "        self.initialize_hidden_states()\n",
    "        self.randomize_edge_attr()\n",
    "        self.add_self_edges()\n",
    "        \n",
    "    def create_grid(self, n, m):\n",
    "        self.graph_type = 'grid'\n",
    "        self.num_nodes = n * m\n",
    "        edges, nx_edges = [], []\n",
    "        \n",
    "        for j in range(m - 1):\n",
    "            edges.append((j, j + 1))\n",
    "            edges.append((j + 1, j))\n",
    "            nx_edges.append((j, j + 1))\n",
    "\n",
    "        for i in range(1, n):\n",
    "            for j in range(m - 1):\n",
    "                edges.append((m*i + j, m*i + j + 1))\n",
    "                edges.append((m*i + j + 1, m*i + j))\n",
    "                nx_edges.append((m*i + j, m*i + j + 1))\n",
    "                edges.append((m*(i-1) + j, m*i + j))\n",
    "                edges.append((m*i + j, m*(i-1) + j))\n",
    "                nx_edges.append((m*(i-1) + j, m*i + j))\n",
    "            edges.append((m*(i-1) + m - 1, m*i + m - 1))\n",
    "            edges.append((m*i + m - 1, m*(i-1) + m - 1))\n",
    "            nx_edges.append((m*(i-1) + m - 1, m*i + m - 1))\n",
    "            \n",
    "        edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "        self.edge_index = edge_index.t().contiguous()\n",
    "        self.nx_graph.add_edges_from(nx_edges)\n",
    "        \n",
    "        self.initialize_node_attr()\n",
    "        self.initialize_hidden_states()\n",
    "        self.randomize_edge_attr()\n",
    "        self.add_self_edges()\n",
    "        \n",
    "    def initialize_node_attr(self, val=0.):\n",
    "        self.x = torch.tensor([val for _ in range(self.num_nodes)]).unsqueeze_(-1)\n",
    "        \n",
    "    def initialize_hidden_states(self, dim=32, val=0.):\n",
    "        self.h = torch.zeros([self.num_nodes, dim])\n",
    "    \n",
    "    def randomize_edge_attr(self, a=0.2, b=1):\n",
    "        self.edge_attr = []\n",
    "        for i, edge in enumerate(self.edge_index[0]):\n",
    "            if i % 2 == 0:\n",
    "                weight = random.uniform(a, b)\n",
    "                self.edge_attr.append(weight)\n",
    "            else:\n",
    "                self.edge_attr.append(weight)\n",
    "                \n",
    "        self.edge_attr = torch.tensor(self.edge_attr, dtype=torch.float).unsqueeze_(-1)\n",
    "        \n",
    "    def add_self_edges(self):\n",
    "        \"\"\"Append edge attributes with ones for every self edge.\n",
    "        We use ones instead of zeros, as these edge weights could somehow multiply messages\n",
    "        in the Processor network (MPNN). Maybe, idk.\n",
    "        \"\"\"\n",
    "        self.edge_index, _ = add_self_loops(self.edge_index, num_nodes=self.num_nodes)\n",
    "        self.edge_attr = torch.cat((self.edge_attr, torch.ones([self.num_nodes, 1],\n",
    "                                                                dtype=torch.float)), dim=0)\n",
    "\n",
    "    def create_adj_list(self):\n",
    "        self.adj_list = defaultdict(list)\n",
    "        sources, targets = self.edge_index\n",
    "        for s, t in zip(sources, targets):\n",
    "            self.adj_list[s.item()].append(t.item())\n",
    "    \n",
    "    def draw(self):\n",
    "        if self.graph_type == 'ladder':\n",
    "            nx.draw_spring(self.nx_graph, with_labels=True)\n",
    "        elif self.graph_type == 'grid':\n",
    "            nx.draw(self.nx_graph, with_labels=True)\n",
    "    \n",
    "    def bfs(self, s):\n",
    "        step_list = []\n",
    "        Step = namedtuple('Step', ['step', 'x'])\n",
    "        if self.adj_list is None:\n",
    "            self.create_adj_list()\n",
    "        self.initialize_node_attr()\n",
    "        #print(self.graph.x)\n",
    "        queue = deque()\n",
    "        queue.append(s)\n",
    "        i = 0\n",
    "        \n",
    "        while queue:\n",
    "            n = queue.popleft()\n",
    "            if self.x[n] == 1:\n",
    "                continue\n",
    "            print(f\"Step {i}: visited node {n}\")\n",
    "            self.x[n] = 1.\n",
    "            #print(self.graph.x)\n",
    "            queue.extend(self.adj_list[n])\n",
    "            step_list.append(Step(i, self.x.clone()))\n",
    "            i += 1\n",
    "                             \n",
    "        return step_list\n",
    "            \n",
    "    def belman_ford(self, s):\n",
    "        if self.adj_list is None:\n",
    "            self.create_adj_list()\n",
    "        if self.edge_attr is None:\n",
    "            self.randomize_edge_attr()\n",
    "        self.initialize_node_attr(math.inf)\n",
    "        self.x[s] = 0.\n",
    "        edge_weights = {}\n",
    "        for edge, weight in zip(self.edge_index.t(), self.edge_attr):\n",
    "            edge_weights[(tuple(map(int, edge)))] = weight.item()\n",
    "            \n",
    "        for _ in range(self.num_nodes - 1):\n",
    "            for edge, weight in edge_weights.items():\n",
    "                src, dest = edge[0], edge[1]\n",
    "                if self.x[src] != math.inf and self.x[src] + weight < self.x[dest]:\n",
    "                    self.x[dest] = self.x[src] + weight\n",
    "                    \n",
    "        # Checking for negative cycles\n",
    "        # Not necessary for our case as edge features are positive\n",
    "        for edge, weight in edge_weights.items():\n",
    "            src, dest = edge[0], edge[1]\n",
    "            if self.x[src] != math.inf and self.x[src] + weight < self.x[dest]:\n",
    "                print(\"Negative cycle detected!\")\n",
    "                \n",
    "        for n, x in enumerate(self.x):\n",
    "            print(f\"Distance to node {n}: {x.item()}\")\n",
    "            \n",
    "        # return queue like in bfs?\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = f\"Graph type: {self.graph_type}\\n\" + \\\n",
    "                f\"Number of nodes: {self.num_nodes}\\n\" + \\\n",
    "                f\"Adjacency list:\\n{self.adj_list}\\n\" + \\\n",
    "                f\"Node features:\\n{self.x}\"\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Neural network which performs linear projection of current\n",
    "    features (x^t) and latent features from previous state (h^t-1)\n",
    "    for each node, and outputs encoded node features (z^t)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=33, out_channel=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lin = nn.Linear(in_channel, out_channel, bias=False)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        z = torch.cat((x, h), dim=1)\n",
    "        z = self.lin(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor(MessagePassing):\n",
    "    \"\"\"Neural network performing message-passing between the nodes.\n",
    "    It takes encoded node features (z^t) and edge features (e^t), and produces latent\n",
    "    features (h^t) for each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=10, hidden_channels=20, out_channels=32):\n",
    "        super(Processor, self).__init__(aggr='max', flow='source_to_target')\n",
    "        self.lin1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    # I need edge features as well, but don't know how tu plug them in\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        #edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # U propagate uvijek saljes edge_index i size, i sve dodatne kwargse koje koristis u funkcijama\n",
    "        # message i update\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        h = edge_attr.view(-1, 1) * x_j\n",
    "        return self.lin1(h)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.lin2(aggr_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Neural network which performs linear projection of encoded\n",
    "    features (z^t) and latent features (h^t) for each node, and produces\n",
    "    node specific outputs (y^t)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=42, out_channel=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lin = nn.Linear(in_channel, out_channel, bias=False)\n",
    "        \n",
    "    def forward(self, z, h):\n",
    "        y = torch.cat((z, h), dim=1)\n",
    "        y = self.lin(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Termination(nn.Module):\n",
    "    \"\"\"Neural network to determine whether an exectuion should be terminated.\n",
    "    Takes in hidden states, and outputs the probability that the algorithm\n",
    "    has finished.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chanels=32, out_channels=1):\n",
    "        super(Termination, self).__init__()\n",
    "        self.lin = nn.Linear(in_chanels, out_channels)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h = torch.mean(h, dim=0)\n",
    "        return torch.sigmoid(self.lin(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "proc = Processor()\n",
    "dec = Decoder()\n",
    "term = Termination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = CustomGraph()\n",
    "G.create_ladder(5)\n",
    "G.x[3] = 1.\n",
    "z = enc(G)\n",
    "h = proc(x=z, edge_index=G.edge_index, edge_attr=G.edge_attr)\n",
    "y = dec(z, h)\n",
    "t = term(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4770], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I have to write the training loop and the rest of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: visited node 5\n",
      "Step 1: visited node 4\n",
      "Step 2: visited node 3\n",
      "Step 3: visited node 7\n",
      "Step 4: visited node 2\n",
      "Step 5: visited node 6\n",
      "Step 6: visited node 1\n",
      "Step 7: visited node 9\n",
      "Step 8: visited node 0\n",
      "Step 9: visited node 8\n",
      "Step 10: visited node 11\n",
      "Step 11: visited node 10\n",
      "Step 12: visited node 13\n",
      "Step 13: visited node 12\n",
      "Step 14: visited node 15\n",
      "Step 15: visited node 14\n",
      "Step 16: visited node 17\n",
      "Step 17: visited node 16\n",
      "Step 18: visited node 19\n",
      "Step 19: visited node 18\n",
      "tensor([[0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0120],\n",
      "        [0.0120],\n",
      "        [0.0046],\n",
      "        [0.0263],\n",
      "        [0.0120],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0213],\n",
      "        [0.0208],\n",
      "        [0.0208],\n",
      "        [0.0172],\n",
      "        [0.0172],\n",
      "        [0.0176],\n",
      "        [0.0208],\n",
      "        [0.0172],\n",
      "        [0.0213],\n",
      "        [0.0208],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213],\n",
      "        [0.0213]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0198],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0200],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0198],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199],\n",
      "        [0.0199]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0189],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n",
      "tensor([[0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190],\n",
      "        [0.0190]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "graph = CustomGraph()\n",
    "graph.create_ladder(10)\n",
    "bfs_queue = graph.bfs(5)\n",
    "graph.initialize_node_attr()\n",
    "graph.graph.x[5] = 1\n",
    "graph.h = torch.zeros([20, 15])\n",
    "enc = Encoder(16, 10)\n",
    "proc = Processor(10, 20, 15)\n",
    "dec = Decoder(25, 1)\n",
    "#term = Terminate(15, 1)  # Kaj se zapravo predaje ovoj mrezi? U kojem se trenu ona tocno pokrece?\n",
    "\n",
    "x = graph.graph.x.clone()\n",
    "h = graph.h.clone()\n",
    "edge_index = graph.graph.edge_index\n",
    "\n",
    "for i in range(len(bfs_queue)):\n",
    "    step = bfs_queue[i]\n",
    "    node_attr = step.x\n",
    "    terminate = True if i == len(bfs_queue)-1 else False\n",
    "    z = enc(x, h)\n",
    "    h = proc(z, edge_index)\n",
    "    y = dec(z, h)\n",
    "    print(y)\n",
    "    x = y.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-ed25aeac3d27>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-ed25aeac3d27>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    P(z, edge_index)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "d = CustomGraph()\n",
    "d.create_ladder(5)\n",
    "d.graph.x = torch.rand((10, 5))\n",
    "\n",
    "x = d.graph.x\n",
    "h = torch.rand((10, 10))\n",
    "edge_index = d.graph.edge_index.t().contiguous()\n",
    "\n",
    "E = Encoder(15, 20)\n",
    "P = Processor(20, 30, 10)\n",
    "D = Decoder(\n",
    "\n",
    "z = E(x, h)\n",
    "P(z, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- [X] Create a queue for algorithm execution\n",
    "- [X] Put named tuples inside with step, x, termination, ...\n",
    "- [ ] Write the training loop\n",
    "- [X] Use edge attributes somehow - easy, just use the default implementation for GCN, not your own shitty implementation\n",
    "- [ ] Define loss\n",
    "- [X] Cluster nodes by their hidden states to see if similar nodes fall within same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing MPNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        #print(edge_index)\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Step 3: Compute normalization\n",
    "        row, col = edge_index\n",
    "        #print(row)\n",
    "        #print(col)\n",
    "        deg = degree(row, num_nodes=x.size(0), dtype=x.dtype)\n",
    "        #print(deg)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        #print(deg_inv_sqrt)\n",
    "        #print(edge_index)\n",
    "        #print()\n",
    "        #print(row.shape)\n",
    "        #print(row)\n",
    "        #print(deg_inv_sqrt.shape)\n",
    "        #print(deg_inv_sqrt)\n",
    "        #print(deg_inv_sqrt[row].shape)\n",
    "        #print(deg_inv_sqrt[row])\n",
    "        \n",
    "        \n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        #print(norm)\n",
    "\n",
    "        # Step 4-6: Start propagating messages.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
    "                              norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        print(\"MESSAGE\")\n",
    "        print(norm.shape, x_j.shape)\n",
    "        b = norm.view(-1, 1)\n",
    "        c = norm.unsqueeze(1)\n",
    "        print(b.shape)\n",
    "        print(c.shape)\n",
    "        #print(x_j)\n",
    "        a = norm.view(-1, 1) * x_j\n",
    "        print(a.shape)\n",
    "        #print(a)\n",
    "        return a\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 6: Return new node embeddings.\n",
    "        print(\"UPDATE\")\n",
    "        print(aggr_out.shape)\n",
    "        #print(aggr_out)\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1406, 0.6591, 0.1697, 0.0669, 0.7949, 0.4376, 0.4883, 0.8505, 0.0106,\n",
      "         0.2651, 0.3136, 0.5625, 0.9660, 0.1673, 0.5228],\n",
      "        [0.5707, 0.3044, 0.3018, 0.4780, 0.5388, 0.1321, 0.4225, 0.1104, 0.8837,\n",
      "         0.4332, 0.1959, 0.1481, 0.6184, 0.3152, 0.5037],\n",
      "        [0.2432, 0.9420, 0.7616, 0.5633, 0.8338, 0.7043, 0.1028, 0.8667, 0.7833,\n",
      "         0.1941, 0.2652, 0.0949, 0.3141, 0.7705, 0.7781],\n",
      "        [0.5991, 0.7170, 0.1504, 0.5622, 0.3782, 0.5089, 0.4149, 0.4699, 0.4020,\n",
      "         0.0640, 0.7310, 0.4683, 0.3597, 0.9037, 0.2889],\n",
      "        [0.2039, 0.1050, 0.1193, 0.7079, 0.0773, 0.7893, 0.2088, 0.8580, 0.4904,\n",
      "         0.6377, 0.9905, 0.5525, 0.1263, 0.0445, 0.2113],\n",
      "        [0.5072, 0.1929, 0.4061, 0.9080, 0.8984, 0.4496, 0.3166, 0.3896, 0.4508,\n",
      "         0.4633, 0.3505, 0.4236, 0.3648, 0.7482, 0.3292],\n",
      "        [0.9047, 0.5309, 0.0570, 0.5134, 0.9501, 0.0661, 0.4373, 0.1053, 0.4888,\n",
      "         0.9868, 0.9355, 0.5249, 0.4799, 0.0290, 0.1758],\n",
      "        [0.5446, 0.9496, 0.7365, 0.1751, 0.8490, 0.5094, 0.8016, 0.8729, 0.4813,\n",
      "         0.8878, 0.3255, 0.1566, 0.6786, 0.8893, 0.7801],\n",
      "        [0.9208, 0.9748, 0.0882, 0.5896, 0.7875, 0.6515, 0.6423, 0.1073, 0.8717,\n",
      "         0.9866, 0.1751, 0.9950, 0.5870, 0.2364, 0.6387],\n",
      "        [0.7525, 0.1755, 0.5688, 0.8220, 0.5570, 0.0511, 0.4944, 0.1918, 0.0164,\n",
      "         0.0459, 0.5658, 0.7340, 0.6622, 0.5603, 0.5650]])\n",
      "tensor([[0, 1],\n",
      "        [1, 0],\n",
      "        [2, 3],\n",
      "        [3, 2],\n",
      "        [2, 0],\n",
      "        [0, 2],\n",
      "        [3, 1],\n",
      "        [1, 3],\n",
      "        [4, 5],\n",
      "        [5, 4],\n",
      "        [4, 2],\n",
      "        [2, 4],\n",
      "        [5, 3],\n",
      "        [3, 5],\n",
      "        [6, 7],\n",
      "        [7, 6],\n",
      "        [6, 4],\n",
      "        [4, 6],\n",
      "        [7, 5],\n",
      "        [5, 7],\n",
      "        [8, 9],\n",
      "        [9, 8],\n",
      "        [8, 6],\n",
      "        [6, 8],\n",
      "        [9, 7],\n",
      "        [7, 9]])\n",
      "torch.Size([10, 15])\n",
      "torch.Size([2, 26])\n"
     ]
    }
   ],
   "source": [
    "d = CustomGraph()\n",
    "d.create_ladder(5)\n",
    "d.graph.x = torch.rand((10, 15))\n",
    "print(d.graph.x)\n",
    "print(d.graph.edge_index)\n",
    "\n",
    "x = d.graph.x\n",
    "edge_index = d.graph.edge_index.t().contiguous()\n",
    "\n",
    "print(x.shape)\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        #edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        #print(x.shape)\n",
    "        x = self.lin(x)\n",
    "        #x = torch.ones((x.size(0), 6))\n",
    "        print(x)\n",
    "        # Step 3: Compute normalization\n",
    "        row, col = edge_index\n",
    "        #print(row, col)\n",
    "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        print(deg_inv_sqrt[row])\n",
    "        #print(deg_inv_sqrt[col])\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        #print(norm)\n",
    "        #print(norm.shape)\n",
    "\n",
    "        # Step 4-6: Start propagating messages.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
    "                              norm=norm, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, norm, edge_attr):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        print(edge_attr.shape)\n",
    "        print(edge_attr.view(-1, 1).shape)\n",
    "        #print(\"x_i\")\n",
    "        #print(x_i)\n",
    "        #print(\"x_j\")\n",
    "        #print(x_j)\n",
    "        #print(x_j.shape)\n",
    "        \n",
    "        ret = edge_attr.view(-1, 1) * x_j\n",
    "        print(ret)\n",
    "        return ret\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 6: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5129, 0.0593, 0.9542],\n",
      "        [0.1566, 0.5738, 0.9336],\n",
      "        [0.0140, 0.7003, 0.3221],\n",
      "        [0.2465, 0.7691, 0.4411],\n",
      "        [0.7643, 0.0187, 0.1459],\n",
      "        [0.4074, 0.5697, 0.0577],\n",
      "        [0.8975, 0.7874, 0.5048],\n",
      "        [0.4921, 0.5120, 0.1873],\n",
      "        [0.7630, 0.9798, 0.7898],\n",
      "        [0.1147, 0.4239, 0.2617],\n",
      "        [0.8958, 0.0777, 0.4051],\n",
      "        [0.6195, 0.4228, 0.9112],\n",
      "        [0.2767, 0.3269, 0.1440],\n",
      "        [0.8497, 0.2596, 0.5317]])\n",
      "tensor([[ 0,  1,  2,  3,  2,  0,  3,  1,  4,  5,  4,  2,  5,  3,  6,  7,  6,  4,\n",
      "          7,  5,  8,  9,  8,  6,  9,  7, 10, 11, 10,  8, 11,  9, 12, 13, 12, 10,\n",
      "         13, 11,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
      "        [ 1,  0,  3,  2,  0,  2,  1,  3,  5,  4,  2,  4,  3,  5,  7,  6,  4,  6,\n",
      "          5,  7,  9,  8,  6,  8,  7,  9, 11, 10,  8, 10,  9, 11, 13, 12, 10, 12,\n",
      "         11, 13,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]])\n"
     ]
    }
   ],
   "source": [
    "graph = CustomGraph()\n",
    "graph.create_ladder(7)\n",
    "graph.x = torch.rand((graph.num_nodes, 3), dtype=torch.float)\n",
    "print(graph.x)\n",
    "print(graph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3094,  0.6654,  0.7840, -0.4709,  0.7415, -0.3710],\n",
      "        [-0.0818,  0.6899,  0.3689, -0.1690,  0.2802, -0.3338],\n",
      "        [ 0.0485,  0.5425, -0.0821,  0.2015,  0.0677, -0.0499],\n",
      "        [ 0.0567,  0.5319,  0.0455,  0.1433,  0.1678, -0.0222],\n",
      "        [ 0.7595,  0.4094,  0.4755, -0.1422,  0.8018,  0.0789],\n",
      "        [ 0.3774,  0.4137,  0.0056,  0.2091,  0.3132,  0.1576],\n",
      "        [ 0.2777,  0.4399,  0.3442,  0.0270,  0.5085,  0.1403],\n",
      "        [ 0.3868,  0.4377,  0.1394,  0.1081,  0.4033,  0.1053],\n",
      "        [ 0.0112,  0.5277,  0.3457,  0.0010,  0.3692,  0.0135],\n",
      "        [ 0.2521,  0.5252,  0.0643,  0.0865,  0.2594, -0.0619],\n",
      "        [ 0.6755,  0.4537,  0.6409, -0.2569,  0.8700,  0.0077],\n",
      "        [ 0.1858,  0.6156,  0.6260, -0.2961,  0.6011, -0.2274],\n",
      "        [ 0.4130,  0.4723,  0.1172,  0.0735,  0.3823,  0.0168],\n",
      "        [ 0.5139,  0.4850,  0.5995, -0.2254,  0.7645, -0.0203]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([0.5774, 0.5774, 0.5000, 0.5000, 0.5000, 0.5774, 0.5000, 0.5774, 0.5000,\n",
      "        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5774, 0.5774, 0.5774, 0.5000,\n",
      "        0.5774, 0.5000, 0.5774, 0.5774, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5774, 0.5774])\n",
      "torch.Size([52, 1])\n",
      "torch.Size([52, 1])\n",
      "tensor([[ 2.7602e-01,  5.9362e-01,  6.9944e-01, -4.2012e-01,  6.6146e-01,\n",
      "         -3.3096e-01],\n",
      "        [-7.2943e-02,  6.1542e-01,  3.2905e-01, -1.5079e-01,  2.5000e-01,\n",
      "         -2.9780e-01],\n",
      "        [ 2.4594e-02,  2.7497e-01, -4.1640e-02,  1.0211e-01,  3.4340e-02,\n",
      "         -2.5303e-02],\n",
      "        [ 2.8741e-02,  2.6960e-01,  2.3052e-02,  7.2623e-02,  8.5035e-02,\n",
      "         -1.1263e-02],\n",
      "        [ 3.5896e-02,  4.0134e-01, -6.0777e-02,  1.4904e-01,  5.0122e-02,\n",
      "         -3.6931e-02],\n",
      "        [ 2.2891e-01,  4.9230e-01,  5.8006e-01, -3.4842e-01,  5.4856e-01,\n",
      "         -2.7447e-01],\n",
      "        [ 5.0582e-02,  4.7449e-01,  4.0571e-02,  1.2781e-01,  1.4966e-01,\n",
      "         -1.9823e-02],\n",
      "        [-7.2943e-02,  6.1541e-01,  3.2905e-01, -1.5079e-01,  2.5000e-01,\n",
      "         -2.9779e-01],\n",
      "        [ 2.3901e-01,  1.2883e-01,  1.4962e-01, -4.4735e-02,  2.5232e-01,\n",
      "          2.4825e-02],\n",
      "        [ 1.1875e-01,  1.3020e-01,  1.7674e-03,  6.5786e-02,  9.8559e-02,\n",
      "          4.9586e-02],\n",
      "        [ 6.2624e-01,  3.3757e-01,  3.9202e-01, -1.1721e-01,  6.6112e-01,\n",
      "          6.5045e-02],\n",
      "        [ 4.0005e-02,  4.4728e-01, -6.7734e-02,  1.6610e-01,  5.5860e-02,\n",
      "         -4.1159e-02],\n",
      "        [ 3.7658e-01,  4.1289e-01,  5.6050e-03,  2.0862e-01,  3.1255e-01,\n",
      "          1.5725e-01],\n",
      "        [ 5.6583e-02,  5.3078e-01,  4.5384e-02,  1.4298e-01,  1.6741e-01,\n",
      "         -2.2175e-02],\n",
      "        [ 1.4859e-01,  2.3538e-01,  1.8419e-01,  1.4450e-02,  2.7208e-01,\n",
      "          7.5053e-02],\n",
      "        [ 2.0695e-01,  2.3420e-01,  7.4601e-02,  5.7867e-02,  2.1577e-01,\n",
      "          5.6348e-02],\n",
      "        [ 1.0610e-01,  1.6807e-01,  1.3152e-01,  1.0318e-02,  1.9427e-01,\n",
      "          5.3591e-02],\n",
      "        [ 2.9019e-01,  1.5642e-01,  1.8165e-01, -5.4314e-02,  3.0635e-01,\n",
      "          3.0140e-02],\n",
      "        [ 1.8869e-01,  2.1354e-01,  6.8018e-02,  5.2761e-02,  1.9673e-01,\n",
      "          5.1376e-02],\n",
      "        [ 1.8410e-01,  2.0185e-01,  2.7401e-03,  1.0199e-01,  1.5280e-01,\n",
      "          7.6875e-02],\n",
      "        [ 9.1271e-03,  4.2921e-01,  2.8117e-01,  7.9655e-04,  3.0029e-01,\n",
      "          1.1010e-02],\n",
      "        [ 2.0508e-01,  4.2724e-01,  5.2289e-02,  7.0338e-02,  2.1098e-01,\n",
      "         -5.0330e-02],\n",
      "        [ 6.5525e-03,  3.0813e-01,  2.0185e-01,  5.7185e-04,  2.1558e-01,\n",
      "          7.9040e-03],\n",
      "        [ 1.6217e-01,  2.5688e-01,  2.0102e-01,  1.5770e-02,  2.9693e-01,\n",
      "          8.1909e-02],\n",
      "        [ 1.1739e-01,  2.4455e-01,  2.9930e-02,  4.0261e-02,  1.2076e-01,\n",
      "         -2.8809e-02],\n",
      "        [ 1.8007e-01,  2.0379e-01,  6.4913e-02,  5.0353e-02,  1.8775e-01,\n",
      "          4.9031e-02],\n",
      "        [ 4.3565e-01,  2.9260e-01,  4.1330e-01, -1.6569e-01,  5.6109e-01,\n",
      "          4.9510e-03],\n",
      "        [ 1.1980e-01,  3.9701e-01,  4.0370e-01, -1.9097e-01,  3.8765e-01,\n",
      "         -1.4664e-01],\n",
      "        [ 2.9153e-01,  1.9580e-01,  2.7657e-01, -1.1088e-01,  3.7547e-01,\n",
      "          3.3131e-03],\n",
      "        [ 4.8425e-03,  2.2772e-01,  1.4918e-01,  4.2262e-04,  1.5932e-01,\n",
      "          5.8413e-03],\n",
      "        [ 7.4165e-02,  2.4578e-01,  2.4992e-01, -1.1823e-01,  2.3999e-01,\n",
      "         -9.0782e-02],\n",
      "        [ 1.0066e-01,  2.0971e-01,  2.5666e-02,  3.4525e-02,  1.0356e-01,\n",
      "         -2.4705e-02],\n",
      "        [ 1.0469e-01,  1.1974e-01,  2.9701e-02,  1.8638e-02,  9.6911e-02,\n",
      "          4.2581e-03],\n",
      "        [ 1.3028e-01,  1.2294e-01,  1.5199e-01, -5.7147e-02,  1.9381e-01,\n",
      "         -5.1367e-03],\n",
      "        [ 2.9026e-01,  3.3200e-01,  8.2350e-02,  5.1677e-02,  2.6870e-01,\n",
      "          1.1806e-02],\n",
      "        [ 4.7481e-01,  3.1890e-01,  4.5045e-01, -1.8059e-01,  6.1152e-01,\n",
      "          5.3960e-03],\n",
      "        [ 3.5368e-01,  3.3377e-01,  4.1262e-01, -1.5514e-01,  5.2615e-01,\n",
      "         -1.3945e-02],\n",
      "        [ 1.2784e-01,  4.2367e-01,  4.3081e-01, -2.0380e-01,  4.1368e-01,\n",
      "         -1.5649e-01],\n",
      "        [ 3.0940e-01,  6.6542e-01,  7.8404e-01, -4.7094e-01,  7.4146e-01,\n",
      "         -3.7099e-01],\n",
      "        [-8.1766e-02,  6.8985e-01,  3.6885e-01, -1.6903e-01,  2.8023e-01,\n",
      "         -3.3381e-01],\n",
      "        [ 4.8519e-02,  5.4247e-01, -8.2149e-02,  2.0145e-01,  6.7748e-02,\n",
      "         -4.9918e-02],\n",
      "        [ 5.6701e-02,  5.3188e-01,  4.5478e-02,  1.4327e-01,  1.6776e-01,\n",
      "         -2.2221e-02],\n",
      "        [ 7.5952e-01,  4.0941e-01,  4.7545e-01, -1.4216e-01,  8.0182e-01,\n",
      "          7.8888e-02],\n",
      "        [ 3.7736e-01,  4.1374e-01,  5.6166e-03,  2.0906e-01,  3.1320e-01,\n",
      "          1.5758e-01],\n",
      "        [ 2.7771e-01,  4.3991e-01,  3.4424e-01,  2.7005e-02,  5.0849e-01,\n",
      "          1.4027e-01],\n",
      "        [ 3.8677e-01,  4.3771e-01,  1.3942e-01,  1.0815e-01,  4.0325e-01,\n",
      "          1.0531e-01],\n",
      "        [ 1.1221e-02,  5.2767e-01,  3.4567e-01,  9.7928e-04,  3.6918e-01,\n",
      "          1.3535e-02],\n",
      "        [ 2.5212e-01,  5.2525e-01,  6.4284e-02,  8.6474e-02,  2.5938e-01,\n",
      "         -6.1876e-02],\n",
      "        [ 6.7552e-01,  4.5370e-01,  6.4086e-01, -2.5693e-01,  8.7003e-01,\n",
      "          7.6770e-03],\n",
      "        [ 1.8576e-01,  6.1560e-01,  6.2597e-01, -2.9612e-01,  6.0109e-01,\n",
      "         -2.2738e-01],\n",
      "        [ 4.1296e-01,  4.7234e-01,  1.1716e-01,  7.3522e-02,  3.8228e-01,\n",
      "          1.6797e-02],\n",
      "        [ 5.1390e-01,  4.8497e-01,  5.9954e-01, -2.2543e-01,  7.6451e-01,\n",
      "         -2.0263e-02]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2724,  1.6822,  1.0523, -0.4727,  1.0416, -0.7057],\n",
       "        [ 0.2448,  1.7580,  1.1089, -0.4613,  1.0914, -0.6846],\n",
       "        [ 0.9324,  1.6419,  0.9130, -0.1916,  1.3625, -0.2706],\n",
       "        [ 0.3849,  1.8351,  0.3385,  0.3032,  0.7646, -0.1881],\n",
       "        [ 1.0244,  1.1550,  0.5410,  0.1000,  1.1505,  0.1409],\n",
       "        [ 0.8616,  1.2869,  0.2686,  0.3601,  0.9297,  0.2116],\n",
       "        [ 0.7814,  1.1387,  0.8023,  0.0311,  1.2462,  0.2347],\n",
       "        [ 0.8368,  1.1195,  0.3563,  0.2648,  0.9489,  0.2284],\n",
       "        [ 0.6700,  1.4076,  0.8755, -0.0238,  1.2526,  0.0484],\n",
       "        [ 0.5155,  1.4040,  0.6603,  0.0194,  0.9874, -0.0926],\n",
       "        [ 1.0904,  1.4104,  1.2761, -0.3958,  1.6857, -0.1213],\n",
       "        [ 1.0758,  1.4517,  1.4776, -0.5824,  1.7919, -0.2611],\n",
       "        [ 1.0180,  0.9142,  0.7196, -0.1642,  1.1876,  0.0171],\n",
       "        [ 0.7464,  1.0284,  1.0601, -0.4106,  1.2751, -0.1725]],\n",
       "       grad_fn=<ScatterAddBackward>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCN = GCNConv(3, 6)\n",
    "GCN(graph.x, graph.edge_index, graph.edge_attr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

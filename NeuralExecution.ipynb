{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = Data()\n",
    "        self.graph_type = None\n",
    "        self.adj_list = None\n",
    "        self.num_nodes = 0\n",
    "        self.nx_graph = nx.Graph()\n",
    "    \n",
    "    def create_ladder(self, n):\n",
    "        self.graph_type = 'ladder'\n",
    "        self.num_nodes = 2 * n\n",
    "        edges = [(0, 1), (1, 0)]\n",
    "        nx_edges = [(0, 1)]\n",
    "        \n",
    "        for i in range(2, 2 * n, 2):\n",
    "            edges.append((i, i + 1))\n",
    "            edges.append((i + 1, i))\n",
    "            nx_edges.append((i, i + 1))\n",
    "            edges.append((i, i - 2))\n",
    "            edges.append((i - 2, i))\n",
    "            nx_edges.append((i, i - 2))\n",
    "            edges.append((i + 1, i - 1))\n",
    "            edges.append((i - 1, i + 1))\n",
    "            nx_edges.append((i + 1, i - 1))\n",
    "            \n",
    "        edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "        self.graph.edge_index = edge_index\n",
    "        self.nx_graph.add_edges_from(nx_edges)\n",
    "        \n",
    "    def create_grid(self, n, m):\n",
    "        self.graph_type = 'grid'\n",
    "        self.num_nodes = n * m\n",
    "        edges, nx_edges = [], []\n",
    "        \n",
    "        for j in range(m - 1):\n",
    "            edges.append((j, j + 1))\n",
    "            edges.append((j + 1, j))\n",
    "            nx_edges.append((j, j + 1))\n",
    "\n",
    "        for i in range(1, n):\n",
    "            for j in range(m - 1):\n",
    "                edges.append((m*i + j, m*i + j + 1))\n",
    "                edges.append((m*i + j + 1, m*i + j))\n",
    "                nx_edges.append((m*i + j, m*i + j + 1))\n",
    "                edges.append((m*(i-1) + j, m*i + j))\n",
    "                edges.append((m*i + j, m*(i-1) + j))\n",
    "                nx_edges.append((m*(i-1) + j, m*i + j))\n",
    "            edges.append((m*(i-1) + m - 1, m*i + m - 1))\n",
    "            edges.append((m*i + m - 1, m*(i-1) + m - 1))\n",
    "            nx_edges.append((m*(i-1) + m - 1, m*i + m - 1))\n",
    "            \n",
    "        edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "        self.graph.edge_index = edge_index\n",
    "        self.nx_graph.add_edges_from(nx_edges)\n",
    "        \n",
    "    def initialize_node_attr(self, val=0.):\n",
    "        self.graph.x = torch.tensor([val for _ in range(self.num_nodes)]).unsqueeze_(-1)\n",
    "    \n",
    "    def randomize_edge_attr(self, a=0.2, b=1):\n",
    "        self.graph.edge_attr = torch.tensor([random.uniform(a, b) for _ in range(len(self.graph.edge_index))], \n",
    "                                            dtype=torch.float).unsqueeze_(-1)\n",
    "    \n",
    "    def create_adj_list(self):\n",
    "        self.adj_list = defaultdict(list)\n",
    "        for edge in self.graph.edge_index:\n",
    "            a, b = edge[0].item(), edge[1].item()\n",
    "            self.adj_list[a].append(b)\n",
    "    \n",
    "    def draw(self):\n",
    "        if self.graph_type == 'ladder':\n",
    "            nx.draw_spring(self.nx_graph, with_labels=True)\n",
    "        elif self.graph_type == 'grid':\n",
    "            nx.draw(self.nx_graph, with_labels=True)\n",
    "    \n",
    "    def bfs(self, s):\n",
    "        if self.adj_list is None:\n",
    "            self.create_adj_list()\n",
    "        self.initialize_node_attr()\n",
    "        queue = deque()\n",
    "        queue.append(s)\n",
    "        step = 0\n",
    "        \n",
    "        while queue:\n",
    "            n = queue.popleft()\n",
    "            if self.graph.x[n] == 1:\n",
    "                continue\n",
    "            print(f\"Step {step}: visited node {n}\")\n",
    "            self.graph.x[n] = 1.\n",
    "            queue.extend(self.adj_list[n])\n",
    "            step += 1\n",
    "            \n",
    "        # return queue with each step?\n",
    "        \n",
    "    def belman_ford(self, s):\n",
    "        if self.adj_list is None:\n",
    "            self.create_adj_list()\n",
    "        if self.graph.edge_attr is None:\n",
    "            self.randomize_edge_attr()\n",
    "        self.initialize_node_attr(math.inf)\n",
    "        self.graph.x[s] = 0.\n",
    "        edge_weights = {}\n",
    "        for edge, weight in zip(self.graph.edge_index, self.graph.edge_attr):\n",
    "            edge_weights[(tuple(map(int, edge)))] = weight.item()\n",
    "            \n",
    "        for _ in range(self.num_nodes - 1):\n",
    "            for edge, weight in edge_weights.items():\n",
    "                src, dest = edge[0], edge[1]\n",
    "                if self.graph.x[src] != math.inf and self.graph.x[src] + weight < self.graph.x[dest]:\n",
    "                    self.graph.x[dest] = self.graph.x[src] + weight\n",
    "                    \n",
    "        # Checking for negative cycles\n",
    "        # Not necessary for our case as edge features are positive\n",
    "        for edge, weight in edge_weights.items():\n",
    "            src, dest = edge[0], edge[1]\n",
    "            if self.graph.x[src] != math.inf and self.graph.x[src] + weight < self.graph.x[dest]:\n",
    "                print(\"Negative cycle detected!\")\n",
    "                \n",
    "        for n, x in enumerate(self.graph.x):\n",
    "            print(f\"Distance to node {n}: {x.item()}\")\n",
    "            \n",
    "        # return queue like in bfs?\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = f\"Graph type: {self.graph_type}\\n\" + \\\n",
    "                f\"Number of nodes: {self.num_nodes}\\n\" + \\\n",
    "                f\"Adjacency list:\\n{self.adj_list}\\n\" + \\\n",
    "                f\"Node features:\\n{self.graph.x}\"\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Neural network which performs linear projection of current\n",
    "    features (x^t) and latent features from previous state (h^t-1)\n",
    "    for each node, and outputs encoded node features (z^t)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=33, out_channel=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lin = nn.Linear(in_channel, out_channel, bias=False)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        z = torch.cat((x, h), dim=1)\n",
    "        z = self.lin(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor(MessagePassing):\n",
    "    \"\"\"Neural network performing message-passing between the nodes.\n",
    "    It takes encoded node features (z^t) and edge features (e^t), and produces latent\n",
    "    features (h^t) for each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoded_channels, hidden_channels, latent_channels):\n",
    "        super(Processor, self).__init__(aggr='max', flow='source_to_target')\n",
    "        self.lin1 = nn.Linear(encoded_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, latent_channels)\n",
    "\n",
    "    # I need edge features as well, but don't know how tu plug them in\n",
    "    def forward(self, z, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # U propagate uvijek saljes edge_index i size, i sve dodatne kwargse koje koristis u funkcijama\n",
    "        # message i update\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=z)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        z = self.lin1(x_j)\n",
    "        return z\n",
    "\n",
    "    def update(self, x):\n",
    "        h = self.lin2(x)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Neural network which performs linear projection of encoded\n",
    "    features (z^t) and latent features (h^t) for each node, and produces\n",
    "    node specific outputs (y^t)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=42, out_channel=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lin = nn.Linear(in_channel, out_channel, bias=False)\n",
    "        \n",
    "    def forward(self, z, h):\n",
    "        y = torch.cat((z, h), dim=1)\n",
    "        y = self.lin(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Termination(nn.Module):\n",
    "    \"\"\"Neural network to determine whether an exectuion should be terminated.\n",
    "    Takes in hidden states, and outputs the probability that the algorithm\n",
    "    has finished.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chanels=32, out_channels=1):\n",
    "        super(Termination, self).__init__()\n",
    "        self.lin = nn.Linear(in_chanels, out_channels)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h = torch.mean(h, dim=0)\n",
    "        return torch.sigmoid(self.lin(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5522], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.rand(3, 5)\n",
    "T = Termination(5, 1)\n",
    "T(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([1, 5])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000, 0.3335, 0.5576, 0.9318, 0.2304],\n",
       "        [0.4302, 0.6794, 0.1346, 0.9331, 0.2009],\n",
       "        [0.4771, 0.2420, 0.3100, 0.4492, 0.2473],\n",
       "        [0.4024, 0.4183, 0.3341, 0.7714, 0.2262]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h.shape)\n",
    "print(_dm.shape)\n",
    "print(type(h))\n",
    "print(type(_dm))\n",
    "_dm = _dm.view(1, -1)\n",
    "print(_dm.shape)\n",
    "torch.cat((h, _dm), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CustomGraph()\n",
    "d.create_ladder(5)\n",
    "d.graph.x = torch.rand((10, 5))\n",
    "\n",
    "x = d.graph.x\n",
    "h = torch.rand((10, 10))\n",
    "edge_index = d.graph.edge_index.t().contiguous()\n",
    "\n",
    "E = Encoder(15, 20)\n",
    "P = Processor(20, 30, 10)\n",
    "D = Decoder(\n",
    "\n",
    "z = E(x, h)\n",
    "P(z, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X Define encoder network f_A for each algorithm A\n",
    "#X     - take current node features x and previous latent features h\n",
    "#X     - produce enocded inputs z\n",
    "# Create processor network P - MPNN\n",
    "#     - take current encoded inputs z and edge features e\n",
    "#     - produce latent node features\n",
    "#X Create decoder network g_A for each algorithm A\n",
    "#X     - take current encoded inputs z and latent features h\n",
    "#X     - produce outputs y\n",
    "# Processor network also needs to make a decision whether to terminate\n",
    "# This is done by algorithm specific termination network T_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Napravi nekakav queue i unutra stavi neke (named) tupleove (step, x, termination, ...)\n",
    "# Dobro si rekao, stavi x u kojem su pohranjena sva trenutna stanja i to usporedjuj u svakom koraku\n",
    "# Provjeri treba li nesto promijeniti u smislu da se stanja updateaju paralalno (samo preko susjedstva)\n",
    "# Iako to i nije toliko bitno, jer ce samo nauceni alg biti malo drugaciji, ali ce raditi istu stvar\n",
    "# Danas napravi i ono kaj je Petar rekao - dva grafa, klasteriraj hidden stateove\n",
    "# Hidden stateovi nekih karakteristcnih nodeova bi se trebali poklapati\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        #print(edge_index)\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Step 3: Compute normalization\n",
    "        row, col = edge_index\n",
    "        #print(row)\n",
    "        #print(col)\n",
    "        deg = degree(row, num_nodes=x.size(0), dtype=x.dtype)\n",
    "        #print(deg)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        #print(deg_inv_sqrt)\n",
    "        #print(edge_index)\n",
    "        #print()\n",
    "        #print(row.shape)\n",
    "        #print(row)\n",
    "        #print(deg_inv_sqrt.shape)\n",
    "        #print(deg_inv_sqrt)\n",
    "        #print(deg_inv_sqrt[row].shape)\n",
    "        #print(deg_inv_sqrt[row])\n",
    "        \n",
    "        \n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        #print(norm)\n",
    "\n",
    "        # Step 4-6: Start propagating messages.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
    "                              norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        print(\"MESSAGE\")\n",
    "        print(norm.shape, x_j.shape)\n",
    "        b = norm.view(-1, 1)\n",
    "        c = norm.unsqueeze(1)\n",
    "        print(b.shape)\n",
    "        print(c.shape)\n",
    "        #print(x_j)\n",
    "        a = norm.view(-1, 1) * x_j\n",
    "        print(a.shape)\n",
    "        #print(a)\n",
    "        return a\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 6: Return new node embeddings.\n",
    "        print(\"UPDATE\")\n",
    "        print(aggr_out.shape)\n",
    "        #print(aggr_out)\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1406, 0.6591, 0.1697, 0.0669, 0.7949, 0.4376, 0.4883, 0.8505, 0.0106,\n",
      "         0.2651, 0.3136, 0.5625, 0.9660, 0.1673, 0.5228],\n",
      "        [0.5707, 0.3044, 0.3018, 0.4780, 0.5388, 0.1321, 0.4225, 0.1104, 0.8837,\n",
      "         0.4332, 0.1959, 0.1481, 0.6184, 0.3152, 0.5037],\n",
      "        [0.2432, 0.9420, 0.7616, 0.5633, 0.8338, 0.7043, 0.1028, 0.8667, 0.7833,\n",
      "         0.1941, 0.2652, 0.0949, 0.3141, 0.7705, 0.7781],\n",
      "        [0.5991, 0.7170, 0.1504, 0.5622, 0.3782, 0.5089, 0.4149, 0.4699, 0.4020,\n",
      "         0.0640, 0.7310, 0.4683, 0.3597, 0.9037, 0.2889],\n",
      "        [0.2039, 0.1050, 0.1193, 0.7079, 0.0773, 0.7893, 0.2088, 0.8580, 0.4904,\n",
      "         0.6377, 0.9905, 0.5525, 0.1263, 0.0445, 0.2113],\n",
      "        [0.5072, 0.1929, 0.4061, 0.9080, 0.8984, 0.4496, 0.3166, 0.3896, 0.4508,\n",
      "         0.4633, 0.3505, 0.4236, 0.3648, 0.7482, 0.3292],\n",
      "        [0.9047, 0.5309, 0.0570, 0.5134, 0.9501, 0.0661, 0.4373, 0.1053, 0.4888,\n",
      "         0.9868, 0.9355, 0.5249, 0.4799, 0.0290, 0.1758],\n",
      "        [0.5446, 0.9496, 0.7365, 0.1751, 0.8490, 0.5094, 0.8016, 0.8729, 0.4813,\n",
      "         0.8878, 0.3255, 0.1566, 0.6786, 0.8893, 0.7801],\n",
      "        [0.9208, 0.9748, 0.0882, 0.5896, 0.7875, 0.6515, 0.6423, 0.1073, 0.8717,\n",
      "         0.9866, 0.1751, 0.9950, 0.5870, 0.2364, 0.6387],\n",
      "        [0.7525, 0.1755, 0.5688, 0.8220, 0.5570, 0.0511, 0.4944, 0.1918, 0.0164,\n",
      "         0.0459, 0.5658, 0.7340, 0.6622, 0.5603, 0.5650]])\n",
      "tensor([[0, 1],\n",
      "        [1, 0],\n",
      "        [2, 3],\n",
      "        [3, 2],\n",
      "        [2, 0],\n",
      "        [0, 2],\n",
      "        [3, 1],\n",
      "        [1, 3],\n",
      "        [4, 5],\n",
      "        [5, 4],\n",
      "        [4, 2],\n",
      "        [2, 4],\n",
      "        [5, 3],\n",
      "        [3, 5],\n",
      "        [6, 7],\n",
      "        [7, 6],\n",
      "        [6, 4],\n",
      "        [4, 6],\n",
      "        [7, 5],\n",
      "        [5, 7],\n",
      "        [8, 9],\n",
      "        [9, 8],\n",
      "        [8, 6],\n",
      "        [6, 8],\n",
      "        [9, 7],\n",
      "        [7, 9]])\n",
      "torch.Size([10, 15])\n",
      "torch.Size([2, 26])\n"
     ]
    }
   ],
   "source": [
    "d = CustomGraph()\n",
    "d.create_ladder(5)\n",
    "d.graph.x = torch.rand((10, 15))\n",
    "print(d.graph.x)\n",
    "print(d.graph.edge_index)\n",
    "\n",
    "x = d.graph.x\n",
    "edge_index = d.graph.edge_index.t().contiguous()\n",
    "\n",
    "print(x.shape)\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESSAGE\n",
      "torch.Size([36]) torch.Size([36, 15])\n",
      "torch.Size([36, 1])\n",
      "torch.Size([36, 1])\n",
      "torch.Size([36, 15])\n",
      "UPDATE\n",
      "torch.Size([10, 15])\n"
     ]
    }
   ],
   "source": [
    "conv = GCNConv(15, 15)\n",
    "x = conv(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1007, -0.4942, -0.3864,  0.2864,  0.3258, -0.2034, -0.0524,  0.1674,\n",
       "         -0.0017, -0.4090, -0.4654,  0.0227,  0.1265, -0.3814,  0.1445],\n",
       "        [-0.0683, -0.4782, -0.2408,  0.3723,  0.2370, -0.1755,  0.0338,  0.0707,\n",
       "          0.1085, -0.4143, -0.3083, -0.0044,  0.0373, -0.2913,  0.1844],\n",
       "        [-0.0501, -0.4270, -0.4408,  0.4282,  0.3975, -0.2402, -0.0827,  0.0800,\n",
       "          0.0392, -0.4487, -0.4702, -0.1703,  0.1890, -0.3226,  0.0918],\n",
       "        [-0.0246, -0.5789, -0.3633,  0.2874,  0.3116, -0.2448, -0.0776,  0.2522,\n",
       "          0.0698, -0.4552, -0.5559, -0.1788,  0.0650, -0.2393,  0.0809],\n",
       "        [-0.1630, -0.4594, -0.4700,  0.2730,  0.3662, -0.3421, -0.1767,  0.2028,\n",
       "          0.0874, -0.3411, -0.5032, -0.2898,  0.1067, -0.1521,  0.1213],\n",
       "        [-0.0617, -0.4119, -0.3368,  0.3303,  0.3137, -0.2586, -0.1469,  0.1412,\n",
       "          0.1226, -0.3848, -0.4314, -0.3293,  0.0183, -0.0392,  0.0463],\n",
       "        [-0.1945, -0.3703, -0.3736,  0.3020,  0.2706, -0.2837, -0.2085,  0.1581,\n",
       "          0.0664, -0.3351, -0.3696, -0.1847, -0.0580, -0.0253,  0.0823],\n",
       "        [-0.2194, -0.5053, -0.3980,  0.2550,  0.3443, -0.2903, -0.2709,  0.2687,\n",
       "          0.0486, -0.3306, -0.4839, -0.2564, -0.1020, -0.1232,  0.0649],\n",
       "        [-0.2568, -0.4900, -0.3531,  0.2877,  0.2493, -0.2816, -0.2581,  0.2617,\n",
       "          0.0070, -0.2680, -0.4441, -0.1287, -0.0845, -0.1562,  0.0235],\n",
       "        [-0.1722, -0.4511, -0.3448,  0.2680,  0.2775, -0.2131, -0.3099,  0.2872,\n",
       "         -0.0626, -0.3133, -0.5184, -0.1473, -0.0974, -0.1159, -0.1030]],\n",
       "       grad_fn=<ScatterAddBackward>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
